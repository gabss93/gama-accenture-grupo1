{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h29zp0GNEes"
   },
   "source": [
    "# **Instalação (apenas na primeira vez de uso)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgPg4I9bEwrj"
   },
   "source": [
    "**1.  Instalar o Java**\n",
    "\n",
    "Pelo terminal bash:  sudo apt install default-jre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.  Instalar o Pyspark** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bP-z8C8gEln7",
    "outputId": "9ee41e83-b46d-45ed-b8a5-8dfa2de2168b"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yumNYYdIBck"
   },
   "source": [
    "**3.  Baixar o arquivo compactado (tgz) do Apache Spark na versão 3.3.2. pelo comando** \" !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz \" \n",
    "\n",
    "**-q** é usado para desativar a saída de mensagens do wget, o que significa que o comando será executado em silêncio, sem exibir mensagens de progresso ou de conclusão\n",
    "\n",
    "**!** indica que o comando será executado diretamente no sistema operacional, em vez de ser interpretado como código Python. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4ibS7BR4E29W"
   },
   "outputs": [],
   "source": [
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvI1bEkTIh0_"
   },
   "source": [
    "**4. Extrair o conteúdo do arquivo \"spark-3.3.2-bin-hadoop3.tgz\" para o diretório atual pelo comando \"!tar -xvzf spark-3.3.2-bin-hadoop3.tgz\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5WyIveoHTS0",
    "outputId": "633fd95e-28ed-4efd-a32d-43d6881b4b87"
   },
   "outputs": [],
   "source": [
    "!tar -xvzf spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNnB9xLjHtt6"
   },
   "source": [
    "**5. Instalar o \"findspark\" :** um pacote Python que ajuda a localizar e configurar automaticamente a biblioteca Apache Spark no ambiente de desenvolvimento. Ele permite que você use o Spark com facilidade em seu ambiente de desenvolvimento local, sem precisar configurar manualmente o caminho do Spark e outras variáveis de ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "990I4Td4Hs6s"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp5UGmAgIzfs"
   },
   "source": [
    "\n",
    "**6.Listar o conteúdo do diretório** \"/usr/lib/jvm/\" **para que ver as versões do Java instaladas no sistema.**\n",
    "\n",
    "**7.Listar o conteúdo do diretório do spark.**\n",
    "\n",
    "**8. Imprimir o diretório atual por pwd**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I7b0iCxJTDG",
    "outputId": "f82f3c8a-1603-471f-9317-ee1b18adb633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default-java  java-1.11.0-openjdk-amd64  java-11-openjdk-amd64\r\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/lib/jvm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMid51xcJieT",
    "outputId": "f9152c34-cef8-4954-ade6-fb5c269e8037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE  R\t    RELEASE  conf  examples  kubernetes  python  yarn\r\n",
      "NOTICE\t README.md  bin      data  jars      licenses\t sbin\r\n"
     ]
    }
   ],
   "source": [
    "!ls spark-3.3.2-bin-hadoop3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jR9ZxisXKQfb",
    "outputId": "84a7b11e-4e39-434d-c99a-a26ac44ff84e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projeto\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGSs8juXf1Jg"
   },
   "source": [
    "**9.Fazer as instalações do ODBC e pyodbc para conexão com banco de dados:** \n",
    "\n",
    "- No terminal bash: instalar o ODBC colocando o código presente no \"UBUNTU\" em https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver15&tabs=ubuntu18-install%2Calpine17-install%2Cdebian8-install%2Credhat7-13-install%2Crhel7-offline \n",
    "- No terminal bash: Instalar \"sudo apt install unixodbc-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjoPsgZ9gRrc",
    "outputId": "0ab403d1-b68c-4528-e65d-964a2d0e720a"
   },
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZswVJm2KYID"
   },
   "source": [
    "# **Iniciar o spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TsaNbNkNKauz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/azureuser/projeto/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYXadi2MNSpE"
   },
   "source": [
    "# **Criar uma sessão do Spark e manipular os CSVs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqYi1o0OZu3M",
    "outputId": "c19b0369-6613-4feb-a8ab-c7cc87589c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "| id|                nome|               email|      data_cadastro|        telefone|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "|  1|       Fabio Rogerio|fabio-rogerio_1@g...|2019-06-20 15:41:28|+55(27)2529-2222|\n",
      "|  3|           visitante|visitante_3@gmail...|2019-06-27 20:03:39|+55(26)2524-2423|\n",
      "|  5|Elisângela Louren...|elisangela-louren...|2019-07-15 11:33:15|+55(27)2921-2822|\n",
      "|  6|         José rafael|jose-rafael_6@gma...|2019-07-15 11:49:16|+55(24)2930-2521|\n",
      "|  7|Jamersom Ferreira...|jamersom-ferreira...|2019-07-15 11:51:43|+55(22)2529-2028|\n",
      "|  8|    Robson Fernandes|robson-fernandes_...|2019-07-16 10:56:22|+55(28)2627-2821|\n",
      "|  9|        Gisele Lucia|gisele-lucia_9@gm...|2019-07-21 12:26:12|+55(24)2023-2626|\n",
      "| 11|Luiz Felipe da silva|luiz-felipe-da-si...|2019-07-21 12:30:48|+55(22)2528-2320|\n",
      "| 12|Leojaine da Silva...|leojaine-da-silva...|2019-07-21 12:33:43|+55(28)2424-2220|\n",
      "| 13|  Sandra Lucia Silva|sandra-lucia-silv...|2019-07-22 00:47:07|+55(30)2924-2322|\n",
      "| 14|Flávio Alexandre ...|flavio-alexandre-...|2019-07-26 16:58:06|+55(20)2322-2825|\n",
      "| 15|         Ailtonjogos|ailtonjogos_15@gm...|2019-07-29 23:22:32|+55(29)2329-2220|\n",
      "| 16|          Fabiojogos|fabiojogos_16@gma...|2019-07-29 23:24:24|+55(25)2120-2523|\n",
      "| 17|                Leto|   leto_17@gmail.com|2019-07-29 23:29:07|+55(29)2129-2829|\n",
      "| 18|   Williams da Silva|williams-da-silva...|2019-07-29 23:31:20|+55(24)2223-2929|\n",
      "| 19|           Falajogos|falajogos_19@gmai...|2019-07-29 23:33:17|+55(22)2029-2024|\n",
      "| 20|               Paulo|  paulo_20@gmail.com|2019-07-29 23:35:25|+55(25)2727-3028|\n",
      "| 21|               Cildo|  cildo_21@gmail.com|2019-07-30 11:40:10|+55(21)2222-2422|\n",
      "| 22|  Leandro Silva lyra|leandro-silva-lyr...|2019-07-30 11:41:35|+55(24)2920-2221|\n",
      "| 23|        Ronaldo isac|ronaldo-isac_23@g...|2019-07-30 11:43:11|+55(21)2421-2128|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+----------+----------+-------------------+\n",
      "| id|cliente_id|     valor|               data|\n",
      "+---+----------+----------+-------------------+\n",
      "|532|         4|       9.9|2020-04-17 14:51:37|\n",
      "|544|       335|      20.0|2020-05-09 23:43:07|\n",
      "|545|       335|      20.0|2020-05-12 13:16:27|\n",
      "|546|       335|      20.0|2020-05-12 13:23:04|\n",
      "|580|       335|      70.0|2020-05-13 17:10:20|\n",
      "|585|       335|       2.0|2020-05-14 20:55:27|\n",
      "|586|       335|      37.5|2020-05-15 02:06:39|\n",
      "|588|       335|     120.6|2020-05-15 15:24:29|\n",
      "|594|       335|    102.06|2020-05-16 19:13:13|\n",
      "|595|       335|      98.4|2020-05-16 19:19:27|\n",
      "|605|       335|    161.82|2020-05-19 18:07:39|\n",
      "|608|       335|    126.27|2020-05-20 17:20:55|\n",
      "|614|       335|  111.9525|2020-05-24 18:01:42|\n",
      "|619|       335|    72.962|2020-05-25 03:12:36|\n",
      "|622|       369|      10.0|2020-06-03 22:58:55|\n",
      "|623|       369|      10.0|2020-06-03 23:11:30|\n",
      "|624|       369|      10.0|2020-06-04 09:01:51|\n",
      "|633|       369|     100.0|2020-06-09 20:39:02|\n",
      "|652|       335|       7.0|2020-06-14 21:22:01|\n",
      "|660|       335|112.284375|2020-06-18 15:35:48|\n",
      "+---+----------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+----------+------+-------------------+\n",
      "| id|cliente_id| valor|               data|\n",
      "+---+----------+------+-------------------+\n",
      "| 26|         4|  -5.0|2019-09-03 19:08:05|\n",
      "| 27|         4|  -5.0|2019-09-03 19:20:02|\n",
      "| 29|        74|  -2.0|2019-09-05 15:08:52|\n",
      "| 30|        74|  -2.0|2019-09-05 15:09:07|\n",
      "| 31|        74|  -2.0|2019-09-05 15:09:30|\n",
      "| 32|        74|  -2.0|2019-09-05 15:09:45|\n",
      "| 33|        74|-100.0|2019-09-05 15:11:11|\n",
      "| 35|        74|  -5.0|2019-09-05 15:17:19|\n",
      "| 40|        74|  -2.0|2019-09-05 20:24:40|\n",
      "| 41|        74|  -2.0|2019-09-05 20:25:14|\n",
      "| 42|        74|  -2.0|2019-09-05 20:26:20|\n",
      "| 46|         4| -23.0|2019-09-16 17:57:40|\n",
      "| 47|         4|  -3.0|2019-09-19 20:32:03|\n",
      "| 48|         4|  -3.0|2019-09-19 20:32:22|\n",
      "| 49|         4|  -3.0|2019-09-20 18:01:53|\n",
      "| 50|         4|  -3.0|2019-09-23 10:40:12|\n",
      "| 51|         4|  -3.0|2019-09-23 10:42:06|\n",
      "| 54|         4|  -3.0|2019-09-23 17:44:56|\n",
      "| 56|         4| -20.0|2019-09-23 22:44:50|\n",
      "| 57|         4|-800.0|2019-09-23 22:46:31|\n",
      "+---+----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Fraudes\").getOrCreate()\n",
    "\n",
    "#CLIENTES\n",
    "# leitura do primeiro CSV com o cabecalho\n",
    "df_client_header = spark.read.csv(\"/home/azureuser/projeto/dados/clientes/clients-001.csv\", sep=';', inferSchema=True, header=True)\n",
    "\n",
    "# leitura dos outros CSV em conjunto\n",
    "file_path_clients = [f\"/home/azureuser/projeto/dados/clientes/clients-00{i}.csv\" for i in range(2, 5)]\n",
    "df_clients = spark.read.csv(file_path_clients, sep=';', inferSchema=True, header=False)\n",
    "\n",
    "# uniao do CSV com cabeçalho com os demais \n",
    "df_clients_final = df_client_header.union(df_clients)\n",
    "df_clientes_order = df_clients_final.orderBy('id')\n",
    "df_clientes_order.show()\n",
    "\n",
    "\n",
    "#TRANSAÇÃO_IN\n",
    "df_transaction_in_header = spark.read.csv(\"/home/azureuser/projeto/dados/transação_in/transaction-in-001.csv\", sep=';', inferSchema=True, header=True)\n",
    "\n",
    "# leitura dos outros CSV em conjunto\n",
    "file_paths_transaction_in = [f\"/home/azureuser/projeto/dados/transação_in/transaction-in-00{i}.csv\" for i in range(2, 10)]\n",
    "df_transaction_in = spark.read.csv(file_paths_transaction_in, sep=';', inferSchema=True, header=False)\n",
    "\n",
    "# uniao do CSV com cabeçalho com os demais \n",
    "df_transaction_in_final = df_transaction_in_header.union(df_transaction_in)\n",
    "df_transaction_in_order = df_transaction_in_final.orderBy('id')\n",
    "df_transaction_in_order.show()\n",
    "\n",
    "\n",
    "#TRANSAÇÃO_OUT\n",
    "df_transaction_out_header = spark.read.csv(\"/home/azureuser/projeto/dados/transação_out/transaction-out-001.csv\", sep=';', inferSchema=True, header=True)\n",
    "\n",
    "# leitura dos outros CSV em conjunto\n",
    "file_paths_transaction_out = [f\"/home/azureuser/projeto/dados/transação_out/transaction-out-{str(i).zfill(3)}.csv\" for i in range(2, 64)]\n",
    "df_transaction_out = spark.read.csv(file_paths_transaction_out, sep=';', inferSchema=True, header=False)\n",
    "\n",
    "# uniao do CSV com cabeçalho com os demais \n",
    "df_transaction_out_final = df_transaction_out_header.union(df_transaction_out)\n",
    "df_transaction_out_order = df_transaction_out_final.orderBy('id')\n",
    "df_transaction_out_order.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataframe único de transações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+-------------------+--------------+\n",
      "|  id|cliente_id|valor|               data|tipo_transacao|\n",
      "+----+----------+-----+-------------------+--------------+\n",
      "|8615|       586|  0.2|2022-01-19 20:12:26|            IN|\n",
      "|8613|       586|  0.2|2022-01-19 20:11:25|            IN|\n",
      "|8611|       586|  0.2|2022-01-19 20:10:05|            IN|\n",
      "|8606|       910|300.0|2022-01-19 19:59:36|            IN|\n",
      "|8604|        76|100.0|2022-01-18 12:48:14|            IN|\n",
      "|8603|        76|100.0|2022-01-18 12:48:04|            IN|\n",
      "|8602|        76|100.0|2022-01-18 12:47:47|            IN|\n",
      "|8601|        76|100.0|2022-01-18 12:47:43|            IN|\n",
      "|8600|        76|100.0|2022-01-18 12:47:39|            IN|\n",
      "|8599|        76|100.0|2022-01-18 12:43:05|            IN|\n",
      "|8598|        76|100.0|2022-01-18 12:42:56|            IN|\n",
      "|8597|        76|100.0|2022-01-18 12:40:28|            IN|\n",
      "|8596|        76|100.0|2022-01-18 12:38:19|            IN|\n",
      "|8595|        76|100.0|2022-01-18 12:37:59|            IN|\n",
      "|8594|        76|100.0|2022-01-18 12:37:29|            IN|\n",
      "|8593|        76|100.0|2022-01-18 12:37:19|            IN|\n",
      "|8592|       907| 10.0|2022-01-18 12:30:26|            IN|\n",
      "|8591|       907| 10.0|2022-01-18 12:30:14|            IN|\n",
      "|8590|       907| 10.0|2022-01-18 12:30:10|            IN|\n",
      "|8589|       907| 10.0|2022-01-18 12:30:04|            IN|\n",
      "+----+----------+-----+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Cria uma nova coluna de tipo de  transação (\"IN\" ou \"OUT\")\n",
    "df_in = df_transaction_in_final.withColumn(\"tipo_transacao\", lit(\"IN\"))\n",
    "df_out = df_transaction_out_final.withColumn(\"tipo_transacao\", lit(\"OUT\"))\n",
    "\n",
    "# Faz a união dos dois dataframes \n",
    "df_transactions = df_in.union(df_out)\n",
    "df_transactions.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Verificando se tem colunas de ID repetidos no dataframe \"transactions\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não há dados duplicados por ID no dataframe.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# contar o número de linhas antes de remover duplicatas\n",
    "total_rows = df_transactions.count()\n",
    "\n",
    "# remover duplicatas por 'id'\n",
    "df_no_duplicates = df_transactions.dropDuplicates(['id'])\n",
    "\n",
    "# contar o número de linhas após a remoção de duplicatas\n",
    "unique_rows = df_no_duplicates.count()\n",
    "\n",
    "# verificar se há duplicatas por 'id'\n",
    "if total_rows > unique_rows:\n",
    "    print(\"Há dados duplicados por ID no dataframe.\")\n",
    "else:\n",
    "    print(\"Não há dados duplicados por ID no dataframe.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Verificando os tipos dos dados nas colunas nos dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- data_cadastro: timestamp (nullable = true)\n",
      " |-- telefone: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- cliente_id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- tipo_transacao: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imprimir os tipos de dado de cada coluna dos dataframes\n",
    "\n",
    "df_clients_final.printSchema()\n",
    "\n",
    "df_transactions.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
